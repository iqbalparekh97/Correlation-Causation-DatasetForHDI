# -*- coding: utf-8 -*-
"""Synthetic Dataset IES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eV4mkIAvOYkxTA_4bRMggFCJJU8uTBHd

**Correlated model Example**

A correlated dataset where the features have strong linear relationships.
A causal dataset where one primary feature is the primary cause for changes in other features.
Both datasets will have a focus on countries.

First, let's consider some features:

GDP: Gross Domestic Product.
Literacy Rate: Percentage of people over the age of 15 who can read and write.
Internet Users: Number of internet users.
Life Expectancy: Average lifespan of citizens.
Unemployment Rate: Percentage of people unemployed.
For the correlated dataset:

The assumptions are GDP will be positively correlated with the Literacy Rate and Internet Users, and negatively correlated with Unemployment Rate. Additionally, it will be positively correlated with Life Expectancy.
For the causal dataset:

**Correlated Data Example**
"""

import numpy as np
import pandas as pd

# List of countries (simplified for the example)
countries = ['USA', 'UK', 'Canada', 'India', 'Germany', 'France', 'Brazil', 'Australia', 'China', 'Japan']
num_samples = 500

# Generating correlated dataset
gdp = np.linspace(1000, 50000, num_samples * len(countries))  # GDP values
literacy_rate = 0.0005 * gdp + np.random.normal(0, 5, num_samples * len(countries))
internet_users = 0.01 * gdp + np.random.normal(0, 500, num_samples * len(countries))
life_expectancy = 40 + 0.001 * gdp + np.random.normal(0, 2, num_samples * len(countries))
unemployment = 50 - 0.0007 * gdp + np.random.normal(0, 2, num_samples * len(countries))

correlated_data = pd.DataFrame({
    'Country': np.tile(countries, num_samples),
    'GDP': gdp,
    'Literacy Rate': literacy_rate,
    'Internet Users': internet_users,
    'Life Expectancy': life_expectancy,
    'Unemployment Rate': unemployment
})

correlated_data

# Compute the correlation matrix
correlation_matrix = correlated_data.corr()
correlation_matrix



import seaborn as sns
import matplotlib.pyplot as plt

# Plotting a heatmap using seaborn
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

correlated_data.to_csv("correlated_data.csv", index=False)

"""**perfect correlation Dataset Example**"""

import numpy as np
import pandas as pd

# Sample size
num_samples = 5000

# Simulating Urban Population (in thousands)
urban_population = np.random.randint(50, 2000, size=num_samples)

# Simulating Public Transport Usage (in thousands) based on Urban Population
public_transport_usage = urban_population + np.random.randint(-50, 50, size=num_samples)

# Simulating Business Establishments based on Urban Population
business_establishments = (urban_population * 0.5) + np.random.randint(-20, 20, size=num_samples)

# Simulating Average Income (in thousands of dollars) based on Business Establishments
avg_income = (business_establishments * 0.4) + np.random.randint(30, 50, size=num_samples)

# Simulating Entertainment Expenditure (in hundreds) based on Average Income
entertainment_expenditure = (avg_income * 0.1) + np.random.randint(5, 15, size=num_samples)

# Combine into DataFrame
causal_data = pd.DataFrame({
    "Urban Population": urban_population,
    "Public Transport Usage": public_transport_usage,
    "Business Establishments": business_establishments,
    "Average Income": avg_income,
    "Entertainment Expenditure": entertainment_expenditure
})

# If you want to add country names, you can use a library like 'Faker' to generate country names.
# Remember that adding all real-world countries and ensuring unique entries will require additional adjustments.

causal_data.head(1000)

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram for each column
for column in causal_data.columns:
    sns.histplot(causal_data[column], kde=True)
    plt.title(f'Distribution of {column}')
    plt.show()

# Calculate correlation matrix
corr_matrix = causal_data.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Compute the correlation matrix
causal_matrix = causal_data.corr()
causal_matrix

causal_data.to_csv("causal_data.csv", index=False)

"""------------------------------------DATA SET 1"""

b2=pd.read_excel("/content/Book2.xlsx")
countries1=b2['Countries'].tolist()
len(countries1)

countries1 = [
    "Afghanistan", "Albania", "Algeria", "Andorra", "Angola",
    "Antigua and Barbuda", "Argentina", "Armenia", "Australia", "Austria",
    "Azerbaijan", "Bahamas", "Bahrain", "Bangladesh", "Barbados",
    "Belarus", "Belgium", "Belize", "Benin", "Bhutan",
    "Bolivia", "Bosnia and Herzegovina", "Botswana", "Brazil", "Brunei",
    "Bulgaria", "Burkina Faso", "Burundi", "Côte d'Ivoire", "Cabo Verde",
    "Cambodia", "Cameroon", "Canada", "Central African Republic", "Chad",
    "Chile", "China", "Colombia", "Comoros", "Congo (Congo-Brazzaville)",
    "Costa Rica", "Croatia", "Cuba", "Cyprus", "Czechia (Czech Republic)",
    "Democratic Republic of the Congo", "Denmark", "Djibouti", "Dominica", "Dominican Republic",
    "Ecuador", "Egypt", "El Salvador", "Equatorial Guinea", "Eritrea",
    "Estonia", "Eswatini (fmr. Swaziland)", "Ethiopia", "Fiji", "Finland",
    "France", "Gabon", "Gambia", "Georgia", "Germany",
    "Ghana", "Greece", "Grenada", "Guatemala", "Guinea",
    "Guinea-Bissau", "Guyana", "Haiti", "Holy See", "Honduras",
    "Hungary", "Iceland", "India", "Indonesia", "Iran",
    "Iraq", "Ireland", "Israel", "Italy", "Jamaica",
    "Japan", "Jordan", "Kazakhstan", "Kenya", "Kiribati",
    "Kuwait", "Kyrgyzstan", "Laos", "Latvia", "Lebanon",
    "Lesotho", "Liberia", "Libya", "Liechtenstein", "Lithuania",
    "Luxembourg", "Madagascar", "Malawi", "Malaysia", "Maldives",
    "Mali", "Malta", "Marshall Islands", "Mauritania", "Mauritius",
    "Mexico", "Micronesia", "Moldova", "Monaco", "Mongolia",
    "Montenegro", "Morocco", "Mozambique", "Myanmar (formerly Burma)", "Namibia",
    "Nauru", "Nepal", "Netherlands", "New Zealand", "Nicaragua",
    "Niger", "Nigeria", "North Korea", "North Macedonia (formerly Macedonia)", "Norway",
    "Oman", "Pakistan", "Palau", "Palestine State", "Panama",
    "Papua New Guinea", "Paraguay", "Peru", "Philippines", "Poland",
    "Portugal", "Qatar", "Romania", "Russia", "Rwanda",
    "Saint Kitts and Nevis", "Saint Lucia", "Saint Vincent and the Grenadines", "Samoa", "San Marino",
    "Sao Tome and Principe", "Saudi Arabia", "Senegal", "Serbia", "Seychelles",
    "Sierra Leone", "Singapore", "Slovakia", "Slovenia", "Solomon Islands",
    "Somalia", "South Africa", "South Korea", "South Sudan", "Spain",
    "Sri Lanka", "Sudan", "Suriname", "Sweden", "Switzerland",
    "Syria", "Tajikistan", "Tanzania", "Thailand", "Timor-Leste",
    "Togo", "Tonga", "Trinidad and Tobago", "Tunisia", "Turkey",
    "Turkmenistan", "Tuvalu", "Uganda", "Ukraine", "United Arab Emirates",
    "United Kingdom", "United States of America", "Uruguay", "Uzbekistan", "Vanuatu",
    "Venezuela", "Vietnam", "Yemen", "Zambia", "Zimbabwe","u"
]
len(countries1)

import pandas as pd
import numpy as np

# Generate a fictional dataset
np.random.seed(0)
countries = countries1
rnd_spending = np.random.uniform(1, 5, 196)
patents = rnd_spending * 300 + np.random.normal(0, 50, 196)

df = pd.DataFrame({
    'Country': countries,
    'R&D_Spending_Percent_of_GDP': rnd_spending,
    'Patents_Filed': patents
})

#df.to_csv('R & D data')

# Split countries randomly into treatment and control groups
treatment_countries = np.random.choice(df['Country'], size=int(0.5 * len(df)), replace=False)
df['Treatment'] = df['Country'].isin(treatment_countries)

# Simulate an artificial boost in R&D spending for the treatment group
boost_factor = 0.2
df.loc[df['Treatment'], 'R&D_Spending_Percent_of_GDP'] *= (1 + boost_factor)

# Simulate the effect of this boost on patents filed
effect_on_patents = 300  # Assuming every 1% increase in R&D leads to 300 more patents
df.loc[df['Treatment'], 'Patents_Filed'] += df['R&D_Spending_Percent_of_GDP'] * effect_on_patents * boost_factor

# Compare the mean number of patents filed between the two groups
mean_treatment = df[df['Treatment']]['Patents_Filed'].mean()
mean_control = df[~df['Treatment']]['Patents_Filed'].mean()
difference = mean_treatment - mean_control

print(f"Mean patents filed (treatment group): {mean_treatment:.2f}")
print(f"Mean patents filed (control group): {mean_control:.2f}")
print(f"Difference in patents filed: {difference:.2f}")

import statsmodels.api as sm

# Add a constant to the model (intercept)
X = sm.add_constant(df['R&D_Spending_Percent_of_GDP'])
Y = df['Patents_Filed']

model = sm.OLS(Y, X).fit()

print(model.summary())

df

sorted_df = df.sort_values(by='R&DSpendingPercentofGDP', ascending=False)
sorted_df
sorted_df['Country']=countries1
sorted_df

sorted_df=sorted_df.drop('Treatment',axis=1)
sorted_df
sorted_df.to_csv('Data_set1.csv',index=False)

df = df.rename(columns={'R&D_Spending_Percent_of_GDP': 'R&DSpendingPercentofGDP', 'Patents_Filed': 'patentsfiled'})
#df.drop(["Treatment"],axis=1, inplace=True)
#df.to_csv('Rd_data.csv')

"""R&D_Spending_Percent_of_GDP" and "Patents_Filed" with a very high R-squared value of 0.982. The coefficient for "R&D_Spending_Percent_of_GDP" is statistically significant, given the very low p-value (P>|t| is practically 0). This suggests that for every unit increase in "R&D_Spending_Percent_of_GDP," "Patents_Filed" increases by about 303.18 units, holding other factors constant"""

import statsmodels.api as sm

# Add a constant to the model (intercept)
X = sm.add_constant(df['R&DSpendingPercentofGDP'])
Y = df['patentsfiled']

model = sm.OLS(Y, X).fit()
from scipy.stats import pearsonr
print(model.summary())
corr_coefficient, p_value = pearsonr(sorted_df['R&D_Spending_Percent_of_GDP'], sorted_df['Patents_Filed'])
corr_coefficient, p_value

"""Interpreting the RCT:
If the RCT is conducted correctly, and the treatment group shows a statistically significant increase in patent filings compared to the control group, then this provides strong evidence for causality. The randomization ensures that both observed and unobserved confounding variables are equally distributed between the two groups, which strengthens the causal claim.this difference of 118.23 would be interpreted as the causal effect of the treatment (in our hypothetical scenario, the artificial boost in R&D spending) on the outcome (number of patents filed)

The regression results will show a strong association between Months_of_Exposure to the nutritional program and Health_Score. Given the simulated data, it's expected that for each month a region is exposed to the program, the average health score will improve by around 2 points.

This simulation, with its RCT design, implies a causal relationship between the exposure to the nutritional program and the improvement in health scores. If this were real data, such a design would allow us to infer that the program has a positive causal effect on health outcomes in these regions

Since this is based on a simulated Randomized Controlled Trial (RCT), the relationship between Months_of_Exposure and Health_Score can be interpreted causally. This means that the nutritional program does indeed have a causal impact on improving health scores, and for every month of exposure, the health score increases by about 1.85 points.

--------------------------------Dataset2
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Random seed for reproducibility
np.random.seed(42)

# Generate the dataset
countries = ['Country_' + str(i) for i in range(229)]

gdp_per_capita = np.random.uniform(1000, 60000, 229)

# Simulate a correlation: countries with higher GDP have more average years of schooling
# Add some noise to make it more realistic
avg_years_of_schooling = gdp_per_capita * 0.0002 + np.random.uniform(-1, 1, 229)

df3 = pd.DataFrame({
    'Country': countries,
    'GDP_per_Capita': gdp_per_capita,
    'Avg_Years_of_Schooling': avg_years_of_schooling
})

# Visualization
sns.scatterplot(data=df3, x='GDP_per_Capita', y='Avg_Years_of_Schooling')
plt.title('GDP per Capita vs. Average Years of Schooling')
plt.show()

# Calculate correlation coefficient
corr_coefficient, p_value = pearsonr(df3['GDP_per_Capita'], df3['Avg_Years_of_Schooling'])
print(f"Correlation Coefficient: {corr_coefficient:.2f}")
print(f"P-value: {p_value:.4f}")

ex=pd.read_csv('export.csv')

countries3=ex['name'].to_list()
sorted_df3 = df3.sort_values(by='GDP_per_Capita', ascending=False)
sorted_df3
sorted_df3['Country']=countries3
sorted_df3
sorted_df3.to_csv('Data_set2.csv',index=False)

corr_coefficient, p_value = pearsonr(sorted_df3['GDP_per_Capita'], sorted_df3['Avg_Years_of_Schooling'])
print(f"Correlation Coefficient: {corr_coefficient:.2f}")
print(f"P-value: {p_value:.4f}")

"""---------------------------------------------------------------------------------------------------------------------------------------------------------------dataset 3

"""

cf=pd.read_excel('coffee.xlsx')
cf=cf.dropna()

countries4=cf['Country'].to_list()
len(countries4)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Random seed for reproducibility
np.random.seed(42)

# Generate the dataset

countries=countries1
avg_annual_temp = np.random.uniform(-10, 40, 190)  # Temperatures ranging from -10°C to 40°C

# Simulate a correlation: countries with lower temperatures have higher coffee consumption
# Adding some noise for realism
coffee_consumption = (avg_annual_temp * -0.5) + np.random.uniform(0, 10, 190) + 40  # Consumption in kg per capita

df4 = pd.DataFrame({
    'Country': countries4,
    'Avg_Annual_Temperature': avg_annual_temp,
    'Coffee_Consumption_kg': coffee_consumption
})

# Visualization
sns.scatterplot(data=df4, x='Avg_Annual_Temperature', y='Coffee_Consumption_kg')
plt.title('Average Annual Temperature vs. Coffee Consumption per Capita')
plt.show()

# Calculate correlation coefficient
corr_coefficient, p_value = pearsonr(df4['Avg_Annual_Temperature'], df4['Coffee_Consumption_kg'])
print(f"Correlation Coefficient: {corr_coefficient:.2f}")
print(f"P-value: {p_value:.4f}")

sorted_df4 = df4.sort_values(by='Avg_Annual_Temperature', ascending=False)
sorted_df4
sorted_df4['Country']=countries4
sorted_df4
sorted_df4.to_csv('Data_set3.csv',index=False)

df4

corr_coefficient, p_value = pearsonr(sorted_df4['Avg_Annual_Temperature'], sorted_df4['Coffee_Consumption_kg'])
corr_coefficient, p_value

# Visualization
sns.scatterplot(data=sorted_df4, x='Avg_Annual_Temperature', y='Coffee_Consumption_kg')
plt.title('Average Annual Temperature vs. Coffee Consumption per Capita')
plt.show()

"""-------------------------------------dataset4

"""

import pandas as pd
import numpy as np

# Generating 200 city names (City1, City2, ..., City200)
cities = [f'City{i}' for i in range(1, 235)]

# Random values for Population and Average Annual Rainfall
 # For reproducibility

Population = np.random.randint(10000, 100000000, 234)  # Random integers for population
Average_Annual_Rainfall = np.random.uniform(10, 200, 234)  # Random floats for rainfall

# Creating the DataFrame
data = pd.DataFrame({
    'Country': cities,
    'Population': Population,
    'Average_Annual_Rainfall': Average_Annual_Rainfall
})

print(data)

correlation = data['Population'].corr(data['Average_Annual_Rainfall'])
print('Pearson correlation coefficient:', correlation)

ct=pd.read_csv('/content/countries-table.csv')
countries5=ct['country'].to_list()
population5=ct['pop2023'].to_list()
print(len(countries5))
sorted_df5 = data.sort_values(by='Population', ascending=False)
sorted_df5
sorted_df5['Country']=countries5

sorted_df5['Population']=population5


sorted_df5
sorted_df5.to_csv('Data_set4.csv',index=False)

correlation = sorted_df5['Population'].corr(sorted_df5['Average_Annual_Rainfall'])
print('Pearson correlation coefficient:', correlation)

"""------------------------------------------------dataset5"""

import pandas as pd
import numpy as np

# Generating 200 city names (City1, City2, ..., City200)
cities = [f'City{i}' for i in range(1, 189)]

# Random values for Annual Tourist Visits and Average Internet Speed
np.random.seed(7) # For reproducibility
Annual_Tourist_Visits = np.random.randint(10000, 10000000, 188)  # Random integers for tourist visits
Fish_Comsumption_percentage = np.random.uniform(5, 100, 188)  # Random floats for internet speed

# Creating the DataFrame
data2 = pd.DataFrame({
    'Country': cities,
    'AnnualTouristVisits(inmillions)': Annual_Tourist_Visits,
    'FishComsumptionpercentage': Fish_Comsumption_percentage
})

print(data2)

correlation = data2['AnnualTouristVisits(inmillions)'].corr(data2['FishComsumptionpercentage'])
print('Pearson correlation coefficient:', correlation)

to=pd.read_excel('tourist.xlsx')

sorted_df6=data2
sorted_df6['Country']=to['Country']
sorted_df6['AnnualTouristVisits(inmillions)']=to['Value']

#sorted_df5['Annual_Tourist_Visits']=population4


sorted_df6
sorted_df6.to_csv('Data_set5.csv',index=False)